#librerias
import os
import csv
import json
import sys
import math
import pandas as pd
from pandas.core.frame import DataFrame
import datetime
import traceback

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

#libreria y conexión al drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Carpetas donde van los datos de las rutas(path), de peso(reported_path) y datos listos (json_path)
path = "/content/drive/MyDrive/Electrologística 2022/Datos Telemetría" #Datos entregados por Empresas telemetria
reported_path="/content/drive/MyDrive/Electrologística 2022/Datos Emp Transporte depurado"#Datos entregados por Emrpesas transporte 
json_path = "/content/drive/MyDrive/Electrologística 2022/json" #carpeta donde quedarán los archivos procesados
reporte_path="/content/drive/MyDrive/Electrologística 2022"

#aplana las curvas para una mejor viusalización
def flatten_data(json_data):
    #Elimina los datos de carga menores a 5% (SOC)
    for x in range(1, len(json_data["SOC"])-1):
        if(json_data["SOC"][x]<5):
            json_data["SOC"][x] = (json_data["SOC"][x-1]+json_data["SOC"][x+1] )/2
        if(json_data["SOC"][x]>100):
            json_data["SOC"][x] = 100
    #Hace que los cambios de carga sean menos bruscos (no cambia mas de 3%) (SOC)
    for x in range(1, len(json_data["SOC"])):
        if(json_data["SOC"][x]-json_data["SOC"][x-1]>3):
            json_data["SOC"][x] = json_data["SOC"][x-1] + 3
        if(json_data["SOC"][x]-json_data["SOC"][x-1]<-3):
            json_data["SOC"][x] = json_data["SOC"][x-1] - 3
        if(json_data["SOC"][x]>100):
            json_data["SOC"][x] = 100
    #aplana datos muy altos o muy bajos (Altura)
    for x in range(1, len(json_data["height"])-1):
        if((json_data["height"][x]>json_data["height"][x-1] and json_data["height"][x]>json_data["height"][x+1]) or (json_data["height"][x]<json_data["height"][x-1] and json_data["height"][x]<json_data["height"][x+1])):
            json_data["height"][x] = (json_data["height"][x-1]+json_data["height"][x+1] )/2
    #hace que los cambios de altura sean menos bruscos (Altura)
    for x in range(1, len(json_data["height"])):
        if(json_data["height"][x]-json_data["height"][x-1]>20):
            json_data["height"][x] = json_data["height"][x-1] + 20
        if(json_data["height"][x]-json_data["height"][x-1]<-20):
            json_data["height"][x] = json_data["height"][x-1] - 20

#Transforma datos a hora en chile. Para aplicación 2022 no aplica. Se solicitó reportar en hora local
def to_clp_time(str_time, diff_time):
  hour = str(int(str_time[11:13])-diff_time).zfill(2)
  rest = str_time[13:]
  return f"{hour}{rest}"

#función que combina las fuentes de datos
def generate_route(route_path, name, weights):
    if "csv" in name:
      date = name[-14:-4]
      name= name[:-4]
    if "csv" not in name:
      date = name[-10:]
      name= name[0:]
    diff_time = 0
    #print(f"weights: {weights}")
    
    
    if date not in weights.keys():
        print("    [!] Error: {} no está en bitácora (no se le asignan pesos)".format(date))
        asigna_pesos = False        
    else:
        asigna_pesos = True
        print("    [>] Éxito: {} se le asignan pesos".format(date))
    

    #datos para pasar a json
    json_data={}
    json_data['latlng'] = []
    json_data['times'] = []
    json_data['odometer'] = []
    json_data['SOC'] = []
    json_data['height'] = []
    json_data['speed'] = 0
    json_data['elevation'] = 0
    json_data['up'] = 0
    json_data['down'] = 0
    json_data['charged'] = 0
    json_data['discharged'] = 0
    
    #cargar archivo de la empresa
    #sensorreader = csv.reader(file)
   # if (len( next(sensorreader))==1):
    sensorreader = pd.read_csv(route_path, delimiter=';', decimal=",", header=0)   #Datos separados por ;
    df = pd.DataFrame(sensorreader)
    df['fecha_hora']=pd.to_datetime(df['fecha_hora'],format="%Y-%m-%d %H:%M:%S")
    df=df.sort_values(by='fecha_hora',ascending=True)
    df['fecha_hora']=df['fecha_hora'].astype(str)
   #     next(sensorreader)
    min_dist = sys.float_info.max
    max_dist = 0
    start_time = 0
    end_time = 0
    n_reads=0
    speed_sum=0.0
   
    #sortedlist = sorted(sensorreader, key=lambda row: row[0], reverse=False) #Ordena por fecha y hora
    for index, row in df.iterrows():
        # if (int(row[0][11:13])<diff_time):
        #     continue
        vel = int(float(row[6]))
        if vel > 0.0:   #suma al acumulado de velocidades la vel instantanea, y suma 1 a las lecturas, si la velocidad es mayor a 0.
            n_reads = n_reads+1
            speed_sum = speed_sum + int(float(row[6]))

        odometer = int(float(row[5]))
        if odometer < min_dist:  #encuentra el tiempo en que inicia la ruta. Primera lectura del odometro
            min_dist = odometer
            start_time = row[0]
        if odometer >= max_dist: #encuentra el tiempo en que termina la ruta. Última lectura del odometro
            max_dist = odometer
            end_time = row[0]
        json_data["latlng"].append([float(row[1]),float(row[2])])
        json_data["times"].append(to_clp_time(row[0],diff_time))
        json_data["odometer"].append(odometer)
        json_data["SOC"].append(float(row[4]))
        json_data["height"].append(int(float(row[3])))

    #datos por ruta
    json_data["initial_odometer"] = min_dist
    json_data["final_odometer"] = max_dist
    json_data["start_time"] = start_time
    json_data["end_time"] = end_time
    json_data["speed"] = speed_sum/n_reads if (n_reads>0) else 0 #velocidad promedio. Si no hay lecturas es cero
    json_data["odometer"] = list(map(lambda x: x-min_dist,json_data["odometer"]))
    if asigna_pesos:
      json_data["weight"] = [-1]*len(json_data["times"])
      json_data["carga_peso"] = [0]*len(json_data["times"])
      json_data["descarga_peso"] = [0]*len(json_data["times"])

    flatten_data(json_data)
    for i in range(len(json_data["height"])-1):
        elevation_diff = json_data["height"][i]-json_data["height"][i+1]
        json_data['elevation'] = json_data['elevation'] + (abs(elevation_diff))  #elevation total (subido - bajado)
        if (elevation_diff > 0):
            json_data['down'] = json_data['down'] + (abs(elevation_diff)) #acumulado de lo bajado
        else:
            json_data['up'] = json_data['up'] + (abs(elevation_diff)) #acumulado de lo subido
    
    for i in range(len(json_data["SOC"])-1):
        dif = json_data["SOC"][i+1] - json_data["SOC"][i]
        if (dif>0):
            json_data['charged'] = json_data['charged'] + dif #acumulado de lo cargado (bateria)
        else:
            json_data['discharged'] = json_data['discharged'] - dif #acumulado de lo descargado (bateria)


    '''
    JOIN ENTRE DATOS DE TELEMETRIA Y DATOS DE BITACORAS
    '''

    if asigna_pesos:
      weight_date = weights[date]
      diff = weight_date['Diff']
      accum_weights = []
      # Hacemos el acumulado de los pesos por hora de carga/descarga
      for idx, val in enumerate(diff):
          if idx == 0:
              accum_weights.append(val)
          else:
              accum_weights.append(max(0,accum_weights[-1]+val))
      
      weight_date['accum_weights'] = accum_weights
      cargas_list = [float(x) for x in weight_date['Carga mercancía'].tolist()]
      descargas_list = [float(x) for x in weight_date['Descarga mercancía'].tolist()]
      # Asignamos cada estado de peso a la hora de telemetria más cercana
      bitacora_hours = weight_date.index.values.tolist()
      for idx, bit_time in enumerate(bitacora_hours):
          min_diff = datetime.timedelta(365)
          idx_selected_bit = ""
          idx_selected_telem = ""
          for idx_telem, time in enumerate(json_data['times']):
              telem_time = datetime.datetime.strptime(time, '%H:%M:%S').time()
              telem_time_delta = datetime.timedelta(hours=telem_time.hour, minutes=telem_time.minute, seconds=telem_time.second)
              bit_time_delta = datetime.timedelta(hours=bit_time.hour, minutes=bit_time.minute, seconds=bit_time.second)
              difference_delta = min(abs(bit_time_delta - telem_time_delta), abs(telem_time_delta - bit_time_delta)) 
              if difference_delta < min_diff:
                  min_diff = difference_delta
                  idx_selected_bit = idx
                  idx_selected_telem = idx_telem
          json_data['weight'][idx_selected_telem] = weight_date['accum_weights'].tolist()[idx_selected_bit] 
          if cargas_list[idx_selected_bit] > 0:
              json_data['carga_peso'][idx_selected_telem] = 1
          elif descargas_list[idx_selected_bit] > 0:
              json_data['descarga_peso'][idx_selected_telem] = 1

      actual_value = 0
      for idx, old_weight in enumerate(json_data['weight']):
          if old_weight != -1:
              actual_value = json_data['weight'][idx] 
          if old_weight == -1:
              json_data['weight'][idx] = actual_value

    json_file = open(f"{json_path}/{name}.json", "w")
    json.dump(json_data,json_file)
    return True


#Inicio del script
routes = {}
reporte={}
folder_list =  os.listdir(path) #listado de carpetas datos telemetria
reported_folder_list = os.listdir(reported_path) #listado de carpetas datos empresa transporte

for folder in folder_list: #for que recorre cada carpeta de datos telemetria
    vehicle_name = folder
    weight_dates = {}
    #genera datos de peso para cada fecha
    if(vehicle_name in reported_folder_list): #si existe carpeta de pesos (datos emp transporte) para el VE:
        weight_filenames = os.listdir(f"{reported_path}/{vehicle_name}")
        
        if (len(weight_filenames)==0):  #si la carpeta para el VE no tiene datos de pesos:
            print("no hay datos de peso para " + vehicle_name)

        else:

            filename = f"{reported_path}/{vehicle_name}/{weight_filenames[0]}"
            print(f"Procesando datos de peso para {vehicle_name}")
            
            if filename.endswith('.xlsx'): #identifica si el dato de emp transporte es un excel .xlsx
                weight_data = pd.read_excel(filename).sort_values(by=['Fecha y hora'])
                
            else: #Sino, buscará un gsheet. El achivo debe tener este formato de nombre y ser único en tu drive 
                worksheet = gc.open(f'Experiencia_Electrologistica_2022_{vehicle_name}').sheet1

                # get_all_values entrega una lista de filas
                rows = worksheet.get_all_values()

                # Convierte a un Dataframe
                weight_data = pd.DataFrame.from_records(rows, columns=['Fecha y hora', 'Conductor', 'Movimiento', 'kilógramos'])
                weight_data = weight_data.iloc[1:]
                weight_data['Fecha y hora']= pd.to_datetime(weight_data['Fecha y hora'], format='%d/%m/%Y %H:%M:%S')

            weight_data['Fecha y hora'] = [d.replace(microsecond = 0) for d in weight_data['Fecha y hora']] #elimina microsegundos si es que los trae
            weight_data = pd.pivot(weight_data, values = 'kilógramos', index=['Fecha y hora','Conductor'], columns = 'Movimiento').reset_index().rename_axis(None, axis=1) #transpone columnas movimiento y kilos
            weight_data['Fecha'] = [d.date() for d in weight_data['Fecha y hora']] #genera nueva columna fecha
            weight_data['Hora'] = [d.time() for d in weight_data['Fecha y hora']] #genera nueva columna hora
            weight_data['Hora'] = weight_data['Hora'].fillna(datetime.datetime.strptime('00:00:00', '%H:%M:%S').time())
            weight_groups = weight_data.groupby('Fecha') #agrupa por fecha
          
            for weight_group_index in weight_groups.groups:
                
                weight_group = weight_groups.get_group(weight_group_index).groupby(['Hora']).sum()
                weight_group['Diff'] = weight_group.apply(lambda x: (float(str(x['Carga mercancía']).replace(",", "." ))-float(str(x['Descarga mercancía']).replace(",", "." ))), axis=1) #calcula diferencia de carga y descarga
                weight_dates[str(weight_group_index)] = weight_group 
    
    else:
        print("no hay datos de peso para " + vehicle_name)
    #itera por cada ruta
    routes[vehicle_name]={'routes':[]}
    reporte[vehicle_name]={'días':[]}
    file_names = [x for x in os.listdir(f"{path}/{folder}")] #if x.endswith('.csv')]
    for name in file_names:
        result = generate_route(f"{path}/{folder}/{name}",name,weight_dates) #llama la función principal, que combina ambas fuentes de datos
        #print(result)
        if (result == True):
            if "csv" in name:
               routes[vehicle_name]['filename']=name[:-15] #si la función se ejecuta correctamente agrega el nombre de la ruta al listado
               routes[vehicle_name]['routes'].append(name[-14:-4])
               reporte[vehicle_name]['días'].append(name[-14:-4])
               #print(routes[vehicle_name]['filename'])
               #print(routes[vehicle_name]['routes'])
            if "csv" not in name:
               routes[vehicle_name]['filename']=name[:-11] #si la función se ejecuta correctamente agrega el nombre de la ruta al listado
               routes[vehicle_name]['routes'].append(name[-10:])
               reporte[vehicle_name]['días'].append(name[-10:])
               #print(routes[vehicle_name]['filename'])
               #print(routes[vehicle_name]['routes'])

#print(routes)
#crea el archivo routes.json y agrega los VE y sus rutas.
routes_file = open(f"{json_path}/routes.json", "w")
json.dump(routes,routes_file)
routes_file.close()
reporte_file=open(f"{json_path}/reporte.txt","w")
for auto in routes[vehicle_name]['filename']:
  reporte_file.write(str(routes[vehicle_name]['filename'])+'\n' +str(routes[vehicle_name]['routes'])+ '\n')
reporte_file.close()
